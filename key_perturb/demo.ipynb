{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## GiLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "model_name = 'llama-7b'\n",
    "beams = 20\n",
    "max_new_tokens = 10\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2, 3\"\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huruihan2022/anaconda3/envs/pAttack/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 31.10s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "from utils import load_model, get_template\n",
    "model, tokenizer, block_name, embedding_name, embed_token_name, _, _ = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpreter import Interpreter\n",
    "interpreter = Interpreter(model, block_name, embed_token_name, embed_token_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token_index': 0, 'optimal_transport': tensor(0.6343, device='cuda:0', dtype=torch.float64)}, {'token_index': 1, 'optimal_transport': tensor(0.5324, device='cuda:0', dtype=torch.float64)}, {'token_index': 2, 'optimal_transport': tensor(0.0768, device='cuda:0', dtype=torch.float64)}, {'token_index': 3, 'optimal_transport': tensor(0.8737, device='cuda:0', dtype=torch.float64)}, {'token_index': 4, 'optimal_transport': tensor(0.1759, device='cuda:0', dtype=torch.float64)}, {'token_index': 5, 'optimal_transport': tensor(0.1701, device='cuda:0', dtype=torch.float64)}, {'token_index': 6, 'optimal_transport': tensor(0.4339, device='cuda:0', dtype=torch.float64)}, {'token_index': 7, 'optimal_transport': tensor(0.2333, device='cuda:0', dtype=torch.float64)}, {'token_index': 8, 'optimal_transport': tensor(0.3202, device='cuda:0', dtype=torch.float64)}, {'token_index': 9, 'optimal_transport': tensor(0.0836, device='cuda:0', dtype=torch.float64)}, {'token_index': 10, 'optimal_transport': tensor(0.2051, device='cuda:0', dtype=torch.float64)}, {'token_index': 11, 'optimal_transport': tensor(0.1736, device='cuda:0', dtype=torch.float64)}, {'token_index': 12, 'optimal_transport': tensor(0.1752, device='cuda:0', dtype=torch.float64)}, {'token_index': 13, 'optimal_transport': tensor(0.0460, device='cuda:0', dtype=torch.float64)}, {'token_index': 14, 'optimal_transport': tensor(0.3330, device='cuda:0', dtype=torch.float64)}, {'token_index': 15, 'optimal_transport': tensor(0.0614, device='cuda:0', dtype=torch.float64)}, {'token_index': 16, 'optimal_transport': tensor(0.3419, device='cuda:0', dtype=torch.float64)}, {'token_index': 17, 'optimal_transport': tensor(0.7779, device='cuda:0', dtype=torch.float64)}, {'token_index': 18, 'optimal_transport': tensor(0.8092, device='cuda:0', dtype=torch.float64)}, {'token_index': 19, 'optimal_transport': tensor(0.6071, device='cuda:0', dtype=torch.float64)}, {'token_index': 20, 'optimal_transport': tensor(0.5705, device='cuda:0', dtype=torch.float64)}, {'token_index': 21, 'optimal_transport': tensor(0.6949, device='cuda:0', dtype=torch.float64)}, {'token_index': 22, 'optimal_transport': tensor(0.8503, device='cuda:0', dtype=torch.float64)}, {'token_index': 23, 'optimal_transport': tensor(0.4499, device='cuda:0', dtype=torch.float64)}, {'token_index': 24, 'optimal_transport': tensor(0.3854, device='cuda:0', dtype=torch.float64)}, {'token_index': 25, 'optimal_transport': tensor(0.5106, device='cuda:0', dtype=torch.float64)}, {'token_index': 26, 'optimal_transport': tensor(0.4674, device='cuda:0', dtype=torch.float64)}, {'token_index': 27, 'optimal_transport': tensor(0.3939, device='cuda:0', dtype=torch.float64)}, {'token_index': 28, 'optimal_transport': tensor(0.7468, device='cuda:0', dtype=torch.float64)}]\n",
      "tensor([2.0317e-04, 6.0652e-05, 4.3747e-05, 3.6020e-05, 2.5288e-05, 2.3849e-05,\n",
      "        2.2327e-05, 2.0442e-05, 1.8890e-05, 1.3194e-05, 1.2424e-05, 7.2247e-06,\n",
      "        7.2188e-06, 7.0323e-06, 6.8564e-06, 6.4107e-06, 6.0773e-06, 5.9415e-06,\n",
      "        5.3421e-06, 4.7362e-06], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from utils import load_model, get_template\n",
    "template = get_template(model_name)\n",
    "\n",
    "beams = 20\n",
    "max_new_tokens = 10\n",
    "\n",
    "query = \"Moreover, he was in dread that if he persisted in his disagreement O'Brien would twist the dial again.\"\n",
    "input_text = f\"{template['prefix']}{query.strip()}{template['postfix']}\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "attributions, probs_sequences = interpreter.interpret_ours(inputs.input_ids, beams, max_new_tokens, \"optimal_transport\")\n",
    "print(attributions)\n",
    "print(probs_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_heatmap_on_text(text, text_encoding, R_text):\n",
    "    # CLS_idx = text_encoding.argmax(dim=-1)\n",
    "    # print(R_text,  CLS_idx, type(R_text), type(CLS_idx))\n",
    "    # R_text = R_text[CLS_idx, 1:CLS_idx]\n",
    "\n",
    "    text_scores = R_text / R_text.sum()\n",
    "    text_scores = text_scores.flatten()\n",
    "    text_tokens = tokenizer.encode(text)\n",
    "    text_tokens_decoded = [tokenizer.decode([a]) for a in text_tokens]\n",
    "    vis_data_records = [visualization.VisualizationDataRecord(text_scores,0,0,0,0,0,text_tokens_decoded,1)]\n",
    "    visualization.visualize_text(vis_data_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6343, 0.5324, 0.0768, 0.8737, 0.1759, 0.1701, 0.4339, 0.2333, 0.3202,\n",
      "        0.0836, 0.2051, 0.1736, 0.1752, 0.0460, 0.3330, 0.0614, 0.3419, 0.7779,\n",
      "        0.8092, 0.6071, 0.5705, 0.6949, 0.8503, 0.4499, 0.3854, 0.5106, 0.4674,\n",
      "        0.3939, 0.7468], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0.8737, 0.8503, 0.8092, 0.7779, 0.7468, 0.6949, 0.6343, 0.6071, 0.5705,\n",
      "        0.5324, 0.5106, 0.4674, 0.4499, 0.4339], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor([ 3, 22, 18, 17, 28, 21,  0, 19, 20,  1, 25, 26, 23,  6],\n",
      "       device='cuda:0')\n",
      "Column 0: Token = ▁he, Max Attribution = 0.873740987708477\n",
      "Column 1: Token = ▁tw, Max Attribution = 0.8503011899245474\n",
      "Column 2: Token = ', Max Attribution = 0.8092123379219845\n",
      "Column 3: Token = ▁O, Max Attribution = 0.7778865854996828\n",
      "Column 4: Token = ., Max Attribution = 0.7467759334811783\n",
      "Column 5: Token = ▁would, Max Attribution = 0.6948849747916637\n",
      "Column 6: Token = <s>, Max Attribution = 0.6342742113948501\n",
      "Column 7: Token = B, Max Attribution = 0.6070649018441803\n",
      "Column 8: Token = rien, Max Attribution = 0.5704572070211715\n",
      "Column 9: Token = ▁Moreover, Max Attribution = 0.5323754044023212\n",
      "Column 10: Token = ▁d, Max Attribution = 0.51062870957676\n",
      "Column 11: Token = ial, Max Attribution = 0.4674317658403193\n",
      "Column 12: Token = ist, Max Attribution = 0.44990073622189375\n",
      "Column 13: Token = ▁dread, Max Attribution = 0.4339094712541406\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0.00</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Moreover                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> he                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dread                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> if                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> he                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pers                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> isted                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> disag                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> re                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ement                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> O                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> B                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rien                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tw                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> d                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ial                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> again                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "attribution_scores = torch.stack([d['optimal_transport'] for d in attributions])\n",
    "print(attribution_scores)\n",
    "max_scores, max_indices = torch.topk(attribution_scores, k=int(0.5 * len(attribution_scores)), dim=0) \n",
    "print(max_scores)\n",
    "print(max_indices)\n",
    "# 将 token ID 转换为 token 字符串\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "# 找出对应的 token 和最大值\n",
    "result = [(tokens[idx.item()], max_scores[i].item()) for i, idx in enumerate(max_indices)]\n",
    "\n",
    "# 打印结果\n",
    "for i, (token, score) in enumerate(result):\n",
    "    print(f\"Column {i}: Token = {token}, Max Attribution = {score}\")\n",
    "\n",
    "show_heatmap_on_text(input_text, inputs.input_ids, attribution_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pAttack)",
   "language": "python",
   "name": "pattack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
